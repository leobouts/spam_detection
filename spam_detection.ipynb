{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <center>PANTELIDIS NIKOS AM2787\n",
    "###  <center>BOUTSIKARIS LEONIDAS AM2776\n",
    "# <center>DATA MINING\n",
    "# <center>ASSIGNMENT 3\n",
    "### <center>EXERCISE 2\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET <br>\n",
    "## TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "with open(\"Data/training_data.txt\", \"r\") as infile:\n",
    "    for i, line in enumerate(infile.readlines()):\n",
    "        temp = line.split('\\t')\n",
    "        train_data.append(temp[1])\n",
    "        train_labels.append(temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorizer = CountVectorizer(ngram_range = (1,2), \n",
    "                                   analyzer = 'word', \n",
    "                                   stop_words = 'english')\n",
    "\n",
    "X_train = train_vectorizer.fit_transform(train_data)\n",
    "X_train = max_abs_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31558"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_id = []\n",
    "with open(\"Data/test_data.txt\", \"r\") as infile:\n",
    "    for i, line in enumerate(infile.readlines()):\n",
    "        temp = line.split('\\t')\n",
    "        test_data.append(temp[1])\n",
    "        test_id.append(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectorizer = CountVectorizer(vocabulary = train_vectorizer.get_feature_names(), \n",
    "                                  ngram_range = (1,2),\n",
    "                                  analyzer = 'word',\n",
    "                                  stop_words = 'english')\n",
    "\n",
    "\n",
    "X_test = test_vectorizer.fit_transform(test_data)\n",
    "X_test = max_abs_scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mNB_clf = MultinomialNB()\n",
    "\n",
    "\n",
    "labels_tmp = [1 if i == \"True\" else 0 for i in train_labels]\n",
    "scores = model_selection.cross_val_score(mNB_clf, X_train, labels_tmp, cv=5, scoring=\"f1\")\n",
    "\n",
    "mNB_clf.fit(X_train, train_labels)\n",
    "\n",
    "y_pred = mNB_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8826185917372099"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", \"w+\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i, l in enumerate(test_id):\n",
    "        f.write(str(l)+\",\"+y_pred[i]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Approaches\n",
    "Firstly we tried vectorize the data with the binary vectorizer that we created for the previous assignment. The results were actually good(around 0.91 score in Kaggle). The problem was that it was hard to configure it. <br><br>\n",
    "Then we decided to try vectorize the data with TFIDF- and Count-Vectorizer. We thought that TFIDF would do the work but we were wrong. We tried:<br>\n",
    "1) With and without stopwords<br>\n",
    "2) 1 and 2 grams of words <br><br>\n",
    "But the results weren't good enough (0.89 - 0.95)\n",
    "<br>\n",
    "In the end we started trying different configurations with the CountVectorizer. The first configuration used a corpus without the stopwords (0.94). Then we tried n-grams. The (1,2) n-grams had the best score (0,96). We tried (1,3) but it gave lower score. Finally we tried the MaxAbsScaler on the vectorized data and we scored (0.97368)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Approach Specs\n",
    "1) CountVectorizer: Removed Stopwords, used (1,2) n-grams <br>\n",
    "2) MaxAbsScaler <br>\n",
    "3) Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Info\n",
    "\n",
    "- Team Name: Boutsikaris, Pantelidis\n",
    "- Score: 0.97368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
